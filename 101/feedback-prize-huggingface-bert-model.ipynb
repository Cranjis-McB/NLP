{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About Competition:\n\n* The goal of this competition is to classify argumentative elements in student writing as \"effective,\" \"adequate,\" or \"ineffective.\"\n\n* The dataset contains argumentative essays written by U.S students in grades 6-12.\n\n* These essays were annotated by expert raters for discourse elements commonly found in argumentative writing such as Lead, Claim, Evidence etc.\n\n* The task is to predict the quality rating of each discourse element.\n\n* Submissions for this track are evaluated using multi-class logarithmic loss.","metadata":{}},{"cell_type":"markdown","source":"## Learning Goal:\n\n* How to use Huggingface Library to train a NLP task.\n\n**References**\n\n1. [Getting started with NLP for absolute beginners by J Howard](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"# Imports\nimport os\n\n# Directories\nDIR = '/kaggle/input/feedback-prize-effectiveness'\nTRAIN_DIR = os.path.join(DIR, 'train')\nTEST_DIR = os.path.join(DIR, 'test')","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:52:54.713513Z","iopub.execute_input":"2023-06-04T04:52:54.713878Z","iopub.status.idle":"2023-06-04T04:52:54.745096Z","shell.execute_reply.started":"2023-06-04T04:52:54.713787Z","shell.execute_reply":"2023-06-04T04:52:54.744427Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Imports\nimport pandas as pd\n\n# Read train and test csv's.\ndf = pd.read_csv(os.path.join(DIR, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(DIR, 'test.csv'))\n\nprint(f'# of training samples: {len(df)}')\nprint(f'# of test samples: {len(test_df)}\\n')\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:52:55.347751Z","iopub.execute_input":"2023-06-04T04:52:55.348032Z","iopub.status.idle":"2023-06-04T04:52:55.643281Z","shell.execute_reply.started":"2023-06-04T04:52:55.347998Z","shell.execute_reply":"2023-06-04T04:52:55.642484Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"# of training samples: 36765\n# of test samples: 10\n\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"       discourse_id      essay_id  \\\n13440  81aafc967519  F2A3BE5019F0   \n34990  1950f537095f  80889A79B329   \n20408  8698e6ba5051  515E8B741A54   \n2654   19617f23c0dc  2FF9836001F4   \n3579   da37345c71fa  3F949C10F639   \n\n                                          discourse_text  \\\n13440  A shadow could have been reflecting off the vi...   \n34990  People who hear only one side of the story wil...   \n20408  Another reason is that this system helps choos...   \n2654   I disagree with this decision because you go t...   \n3579   So that is why I think that the author wants u...   \n\n             discourse_type discourse_effectiveness  \n13440              Evidence             Ineffective  \n34990              Evidence                Adequate  \n20408          Counterclaim             Ineffective  \n2654               Position               Effective  \n3579   Concluding Statement                Adequate  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>discourse_id</th>\n      <th>essay_id</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_effectiveness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13440</th>\n      <td>81aafc967519</td>\n      <td>F2A3BE5019F0</td>\n      <td>A shadow could have been reflecting off the vi...</td>\n      <td>Evidence</td>\n      <td>Ineffective</td>\n    </tr>\n    <tr>\n      <th>34990</th>\n      <td>1950f537095f</td>\n      <td>80889A79B329</td>\n      <td>People who hear only one side of the story wil...</td>\n      <td>Evidence</td>\n      <td>Adequate</td>\n    </tr>\n    <tr>\n      <th>20408</th>\n      <td>8698e6ba5051</td>\n      <td>515E8B741A54</td>\n      <td>Another reason is that this system helps choos...</td>\n      <td>Counterclaim</td>\n      <td>Ineffective</td>\n    </tr>\n    <tr>\n      <th>2654</th>\n      <td>19617f23c0dc</td>\n      <td>2FF9836001F4</td>\n      <td>I disagree with this decision because you go t...</td>\n      <td>Position</td>\n      <td>Effective</td>\n    </tr>\n    <tr>\n      <th>3579</th>\n      <td>da37345c71fa</td>\n      <td>3F949C10F639</td>\n      <td>So that is why I think that the author wants u...</td>\n      <td>Concluding Statement</td>\n      <td>Adequate</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing\n​\n* Analyse the statistic of the number of words in **discourse_text**\n* Concat the discourse_type and di","metadata":{}},{"cell_type":"code","source":"df['text_length'] = df['discourse_text'].apply(lambda x: len(x.split(' ')))\ndf['text_length'].describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:52:58.105663Z","iopub.execute_input":"2023-06-04T04:52:58.106258Z","iopub.status.idle":"2023-06-04T04:52:58.238602Z","shell.execute_reply.started":"2023-06-04T04:52:58.106218Z","shell.execute_reply":"2023-06-04T04:52:58.237920Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"count    36765.000000\nmean        45.721637\nstd         46.641451\nmin          2.000000\n25%         17.000000\n50%         29.000000\n75%         58.000000\nmax        831.000000\nName: text_length, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"**SURPRISING!!!**\n\n* Minimum Length of disclosure_text is 2, and Maximum is 831.\n* However Mean is 45 with standard deviation of 46, which mean most of the disclosure_text have length < 100.","metadata":{}},{"cell_type":"code","source":"# Disclosure Text with length = 2\ndf[df['text_length']==2]","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:52:58.308505Z","iopub.execute_input":"2023-06-04T04:52:58.308772Z","iopub.status.idle":"2023-06-04T04:52:58.327116Z","shell.execute_reply.started":"2023-06-04T04:52:58.308736Z","shell.execute_reply":"2023-06-04T04:52:58.326289Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       discourse_id      essay_id   discourse_text discourse_type  \\\n11     cc921c5cfda4  00944C693682         stress.           Claim   \n452    1ab1030c639a  0A5B8761B187        Disagree        Position   \n1397   210f8f088aa4  1B4E66B0BE0A      pollution.           Claim   \n1571   e18b753a740a  1DC6485ABFF6       interest,           Claim   \n1572   91b5849cdbed  1DC6485ABFF6  funds/workers.           Claim   \n...             ...           ...              ...            ...   \n35968  4a76afecac31  C8FB2508978A         choices           Claim   \n35969  d9c17f7d8b7a  C8FB2508978A       opinions,           Claim   \n35973  9b72380e4fc2  C8FB2508978A       opinions,           Claim   \n35975  247d1c922753  C8FB2508978A        choices,           Claim   \n36549  a54c4c66b7cb  EDDFFD34DBD4       opinions.           Claim   \n\n      discourse_effectiveness  text_length  \n11                   Adequate            2  \n452               Ineffective            2  \n1397                 Adequate            2  \n1571              Ineffective            2  \n1572              Ineffective            2  \n...                       ...          ...  \n35968             Ineffective            2  \n35969                Adequate            2  \n35973             Ineffective            2  \n35975             Ineffective            2  \n36549             Ineffective            2  \n\n[62 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>discourse_id</th>\n      <th>essay_id</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_effectiveness</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>cc921c5cfda4</td>\n      <td>00944C693682</td>\n      <td>stress.</td>\n      <td>Claim</td>\n      <td>Adequate</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>1ab1030c639a</td>\n      <td>0A5B8761B187</td>\n      <td>Disagree</td>\n      <td>Position</td>\n      <td>Ineffective</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1397</th>\n      <td>210f8f088aa4</td>\n      <td>1B4E66B0BE0A</td>\n      <td>pollution.</td>\n      <td>Claim</td>\n      <td>Adequate</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1571</th>\n      <td>e18b753a740a</td>\n      <td>1DC6485ABFF6</td>\n      <td>interest,</td>\n      <td>Claim</td>\n      <td>Ineffective</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1572</th>\n      <td>91b5849cdbed</td>\n      <td>1DC6485ABFF6</td>\n      <td>funds/workers.</td>\n      <td>Claim</td>\n      <td>Ineffective</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>35968</th>\n      <td>4a76afecac31</td>\n      <td>C8FB2508978A</td>\n      <td>choices</td>\n      <td>Claim</td>\n      <td>Ineffective</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>35969</th>\n      <td>d9c17f7d8b7a</td>\n      <td>C8FB2508978A</td>\n      <td>opinions,</td>\n      <td>Claim</td>\n      <td>Adequate</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>35973</th>\n      <td>9b72380e4fc2</td>\n      <td>C8FB2508978A</td>\n      <td>opinions,</td>\n      <td>Claim</td>\n      <td>Ineffective</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>35975</th>\n      <td>247d1c922753</td>\n      <td>C8FB2508978A</td>\n      <td>choices,</td>\n      <td>Claim</td>\n      <td>Ineffective</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>36549</th>\n      <td>a54c4c66b7cb</td>\n      <td>EDDFFD34DBD4</td>\n      <td>opinions.</td>\n      <td>Claim</td>\n      <td>Ineffective</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>62 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"thresholds = [5, 10, 20, 40, 60, 80, 100, 1000]\n\nfor thr in thresholds:\n    num_of_samples = len(df[df['text_length'] <= thr])\n    print(f'# of samples with text length {thr} or lesser: {num_of_samples}')","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:52:59.268034Z","iopub.execute_input":"2023-06-04T04:52:59.268790Z","iopub.status.idle":"2023-06-04T04:52:59.301805Z","shell.execute_reply.started":"2023-06-04T04:52:59.268738Z","shell.execute_reply":"2023-06-04T04:52:59.300946Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"# of samples with text length 5 or lesser: 618\n# of samples with text length 10 or lesser: 3680\n# of samples with text length 20 or lesser: 12620\n# of samples with text length 40 or lesser: 23090\n# of samples with text length 60 or lesser: 27990\n# of samples with text length 80 or lesser: 30973\n# of samples with text length 100 or lesser: 32940\n# of samples with text length 1000 or lesser: 36765\n","output_type":"stream"}]},{"cell_type":"code","source":"# Concat discourse_text + discourse_type to form the input.\ndf['input'] = 'TEXT1: ' + df.discourse_text + '; TEXT2: ' + df.discourse_type\ndf.input.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:52:59.922737Z","iopub.execute_input":"2023-06-04T04:52:59.923240Z","iopub.status.idle":"2023-06-04T04:52:59.970735Z","shell.execute_reply.started":"2023-06-04T04:52:59.923197Z","shell.execute_reply":"2023-06-04T04:52:59.969828Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0    TEXT1: Hi, i'm Isaac, i'm going to be writing ...\n1    TEXT1: On my perspective, I think that the fac...\n2    TEXT1: I think that the face is a natural land...\n3    TEXT1: If life was on Mars, we would know by n...\n4    TEXT1: People thought that the face was formed...\nName: input, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# create a map of the expected ids to their labels with id2label and label2id:\nid2label = {0: \"Adequate\", 1: \"Ineffective\", 2:\"Effective\"}\nlabel2id = {\"Adequate\": 0, \"Ineffective\": 1, \"Effective\":2}\n\ndf['labels'] = df['discourse_effectiveness'].apply(lambda x: label2id[x])\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:00.712896Z","iopub.execute_input":"2023-06-04T04:53:00.713466Z","iopub.status.idle":"2023-06-04T04:53:00.762247Z","shell.execute_reply.started":"2023-06-04T04:53:00.713425Z","shell.execute_reply":"2023-06-04T04:53:00.761401Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       discourse_id      essay_id  \\\n2364   00821551d0ca  2B7A8D15B50C   \n7600   0344a0b41b00  88FD7FAAFA90   \n31070  fee94770bbee  E48B9182B257   \n8692   a814a90ca57b  9CE197B2A9F9   \n9897   6c5aae0257b1  B2426E4674E7   \n\n                                          discourse_text discourse_type  \\\n2364   because of the pictures we took we could not s...       Evidence   \n7600         the students will enjoy doing the project.           Claim   \n31070  This writer is implying that the small state v...       Evidence   \n8692         people shuld participate in the program. \\n       Position   \n9897   The author also states, \"Some simplified elect...       Evidence   \n\n      discourse_effectiveness  text_length  \\\n2364                 Adequate           42   \n7600                 Adequate            8   \n31070               Effective           66   \n8692                 Adequate            7   \n9897                Effective           97   \n\n                                                   input  labels  \n2364   TEXT1: because of the pictures we took we coul...       0  \n7600   TEXT1: the students will enjoy doing the proje...       0  \n31070  TEXT1: This writer is implying that the small ...       2  \n8692   TEXT1: people shuld participate in the program...       0  \n9897   TEXT1: The author also states, \"Some simplifie...       2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>discourse_id</th>\n      <th>essay_id</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_effectiveness</th>\n      <th>text_length</th>\n      <th>input</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2364</th>\n      <td>00821551d0ca</td>\n      <td>2B7A8D15B50C</td>\n      <td>because of the pictures we took we could not s...</td>\n      <td>Evidence</td>\n      <td>Adequate</td>\n      <td>42</td>\n      <td>TEXT1: because of the pictures we took we coul...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7600</th>\n      <td>0344a0b41b00</td>\n      <td>88FD7FAAFA90</td>\n      <td>the students will enjoy doing the project.</td>\n      <td>Claim</td>\n      <td>Adequate</td>\n      <td>8</td>\n      <td>TEXT1: the students will enjoy doing the proje...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31070</th>\n      <td>fee94770bbee</td>\n      <td>E48B9182B257</td>\n      <td>This writer is implying that the small state v...</td>\n      <td>Evidence</td>\n      <td>Effective</td>\n      <td>66</td>\n      <td>TEXT1: This writer is implying that the small ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8692</th>\n      <td>a814a90ca57b</td>\n      <td>9CE197B2A9F9</td>\n      <td>people shuld participate in the program. \\n</td>\n      <td>Position</td>\n      <td>Adequate</td>\n      <td>7</td>\n      <td>TEXT1: people shuld participate in the program...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9897</th>\n      <td>6c5aae0257b1</td>\n      <td>B2426E4674E7</td>\n      <td>The author also states, \"Some simplified elect...</td>\n      <td>Evidence</td>\n      <td>Effective</td>\n      <td>97</td>\n      <td>TEXT1: The author also states, \"Some simplifie...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenization\n\nBut we can't pass the texts directly into a model. A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:\n\n* **Tokenization**: Split each text up into words (or actually, as we'll see, into tokens)\n* **Numericalization**: Convert each word (or token) into a number.","metadata":{}},{"cell_type":"code","source":"# Transformers uses dataset object to store data.\nfrom datasets import Dataset,DatasetDict\n\nds = Dataset.from_pandas(df)\nds","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:02.747754Z","iopub.execute_input":"2023-06-04T04:53:02.748028Z","iopub.status.idle":"2023-06-04T04:53:05.403908Z","shell.execute_reply.started":"2023-06-04T04:53:02.747996Z","shell.execute_reply":"2023-06-04T04:53:05.403047Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['discourse_id', 'essay_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text_length', 'input', 'labels'],\n    num_rows: 36765\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Choose a model\nmodel_nm = 'bert-base-uncased'","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:05.827718Z","iopub.execute_input":"2023-06-04T04:53:05.828388Z","iopub.status.idle":"2023-06-04T04:53:05.835099Z","shell.execute_reply.started":"2023-06-04T04:53:05.828349Z","shell.execute_reply":"2023-06-04T04:53:05.834181Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# AutoTokenizer will create a tokenizer appropriate for a given model:\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:07.847547Z","iopub.execute_input":"2023-06-04T04:53:07.847857Z","iopub.status.idle":"2023-06-04T04:53:10.301761Z","shell.execute_reply.started":"2023-06-04T04:53:07.847820Z","shell.execute_reply":"2023-06-04T04:53:10.300980Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca84a08348343af855b0820df6cc341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a03f50a16f43f08405da719907b662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c7b053ee3c44ec084c20ba8d28e31f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac70cc1a83c42f2abde0f5f403b250a"}},"metadata":{}}]},{"cell_type":"code","source":"# Uncommon words will be split into pieces. \n# The start of a new word is represented by ▁:\ntokz.tokenize(\"G'day folks, I'm Jeremy from fast.ai!\")","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:14.547806Z","iopub.execute_input":"2023-06-04T04:53:14.548106Z","iopub.status.idle":"2023-06-04T04:53:14.561853Z","shell.execute_reply.started":"2023-06-04T04:53:14.548069Z","shell.execute_reply":"2023-06-04T04:53:14.561132Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['g',\n \"'\",\n 'day',\n 'folks',\n ',',\n 'i',\n \"'\",\n 'm',\n 'jeremy',\n 'from',\n 'fast',\n '.',\n 'ai',\n '!']"},"metadata":{}}]},{"cell_type":"code","source":"# Here's a simple function which tokenizes our inputs\ndef tok_func(x): return tokz(x[\"input\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:15.588002Z","iopub.execute_input":"2023-06-04T04:53:15.588680Z","iopub.status.idle":"2023-06-04T04:53:15.593219Z","shell.execute_reply.started":"2023-06-04T04:53:15.588642Z","shell.execute_reply":"2023-06-04T04:53:15.592218Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# To run this quickly in parallel on every row in our dataset, use map:\ntok_ds = ds.map(tok_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:17.518014Z","iopub.execute_input":"2023-06-04T04:53:17.518752Z","iopub.status.idle":"2023-06-04T04:53:25.354863Z","shell.execute_reply.started":"2023-06-04T04:53:17.518714Z","shell.execute_reply":"2023-06-04T04:53:25.353780Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/37 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b8a671f488848c08f699741bf05b2af"}},"metadata":{}}]},{"cell_type":"code","source":"tok_ds","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:26.767932Z","iopub.execute_input":"2023-06-04T04:53:26.768520Z","iopub.status.idle":"2023-06-04T04:53:26.775424Z","shell.execute_reply.started":"2023-06-04T04:53:26.768480Z","shell.execute_reply":"2023-06-04T04:53:26.774723Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['discourse_id', 'essay_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text_length', 'input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36765\n})"},"metadata":{}}]},{"cell_type":"code","source":"row = tok_ds[0]\nprint(row['input'])\nprint(row['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:31.782950Z","iopub.execute_input":"2023-06-04T04:53:31.783579Z","iopub.status.idle":"2023-06-04T04:53:31.790855Z","shell.execute_reply.started":"2023-06-04T04:53:31.783530Z","shell.execute_reply":"2023-06-04T04:53:31.789768Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"TEXT1: Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform. ; TEXT2: Lead\n[101, 3793, 2487, 1024, 7632, 1010, 1045, 1005, 1049, 7527, 1010, 1045, 1005, 1049, 2183, 2000, 2022, 3015, 2055, 2129, 2023, 2227, 2006, 7733, 2003, 1037, 3019, 2455, 14192, 2030, 2065, 2045, 2003, 2166, 2006, 7733, 2008, 2081, 2009, 1012, 1996, 2466, 2003, 2055, 2129, 9274, 2165, 1037, 3861, 1997, 7733, 1998, 1037, 2227, 2001, 2464, 2006, 1996, 4774, 1012, 9274, 2987, 1005, 1056, 2113, 2065, 1996, 2455, 14192, 2001, 2580, 2011, 2166, 2006, 7733, 1010, 2030, 2065, 2009, 2003, 2074, 1037, 3019, 2455, 14192, 1012, 1025, 3793, 2475, 1024, 2599, 102]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokz.convert_ids_to_tokens(101)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:34.482557Z","iopub.execute_input":"2023-06-04T04:53:34.483518Z","iopub.status.idle":"2023-06-04T04:53:34.489843Z","shell.execute_reply.started":"2023-06-04T04:53:34.483465Z","shell.execute_reply":"2023-06-04T04:53:34.489115Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'[CLS]'"},"metadata":{}}]},{"cell_type":"markdown","source":"* The CLS token, short for \"classification token,\" is a special token used in Transformer-based models, such as BERT\n*  It is used as the first token in the input sequence, and it carries important information for classification tasks.\n* The final hidden state corresponding to the CLS token is often used as the input to a classifier layer or for downstream tasks.","metadata":{}},{"cell_type":"code","source":"# Train-Valid Split.\ndds = tok_ds.train_test_split(0.25, seed=42)\ndds","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:37.907890Z","iopub.execute_input":"2023-06-04T04:53:37.908743Z","iopub.status.idle":"2023-06-04T04:53:37.936017Z","shell.execute_reply.started":"2023-06-04T04:53:37.908700Z","shell.execute_reply":"2023-06-04T04:53:37.935107Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['discourse_id', 'essay_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text_length', 'input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 27573\n    })\n    test: Dataset({\n        features: ['discourse_id', 'essay_id', 'discourse_text', 'discourse_type', 'discourse_effectiveness', 'text_length', 'input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9192\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, \n# instead of padding the whole dataset to the maximum length.\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokz)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:39.122654Z","iopub.execute_input":"2023-06-04T04:53:39.122947Z","iopub.status.idle":"2023-06-04T04:53:45.204728Z","shell.execute_reply.started":"2023-06-04T04:53:39.122913Z","shell.execute_reply":"2023-06-04T04:53:45.203952Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Define Metric (multi-class logarithmic loss)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import log_loss, accuracy_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred.predictions, eval_pred.label_ids\n\n    # Apply softmax to obtain class probabilities\n    probabilities = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)\n\n    # Compute logarithmic loss\n    loss = log_loss(labels, probabilities)\n\n    # Compute accuracy\n    predicted_labels = np.argmax(probabilities, axis=1)\n    accuracy = accuracy_score(labels, predicted_labels)\n\n    return {'log_loss': round(loss, 4), 'accuracy': round(accuracy, 4)}","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:49.292753Z","iopub.execute_input":"2023-06-04T04:53:49.293025Z","iopub.status.idle":"2023-06-04T04:53:49.299677Z","shell.execute_reply.started":"2023-06-04T04:53:49.292993Z","shell.execute_reply":"2023-06-04T04:53:49.298957Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments,Trainer","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:52.967762Z","iopub.execute_input":"2023-06-04T04:53:52.968044Z","iopub.status.idle":"2023-06-04T04:53:53.152415Z","shell.execute_reply.started":"2023-06-04T04:53:52.968011Z","shell.execute_reply":"2023-06-04T04:53:53.151624Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nbs = 16\nepochs = 3\nlr = 8e-5","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:53:56.107792Z","iopub.execute_input":"2023-06-04T04:53:56.108107Z","iopub.status.idle":"2023-06-04T04:53:56.113021Z","shell.execute_reply.started":"2023-06-04T04:53:56.108068Z","shell.execute_reply":"2023-06-04T04:53:56.111958Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments('outputs', \n                         learning_rate=lr, \n                         warmup_ratio=0.1, \n                         lr_scheduler_type='cosine', \n                         fp16=True,\n                         evaluation_strategy=\"epoch\", \n                         per_device_train_batch_size=bs, \n                         per_device_eval_batch_size=bs*2,\n                         num_train_epochs=epochs, \n                         weight_decay=0.01,\n                         report_to='none'\n                        )","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:54:01.027946Z","iopub.execute_input":"2023-06-04T04:54:01.028736Z","iopub.status.idle":"2023-06-04T04:54:01.130475Z","shell.execute_reply.started":"2023-06-04T04:54:01.028696Z","shell.execute_reply":"2023-06-04T04:54:01.129510Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"We can now create our model, and Trainer, which is a class which combines the data and model together (just like Learner in fastai):","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=3, id2label=id2label, label2id=label2id)\ntrainer = Trainer(model, \n                  args, \n                  train_dataset=dds['train'], \n                  eval_dataset=dds['test'],\n                  tokenizer=tokz,\n                  data_collator=data_collator, \n                  compute_metrics=compute_metrics\n                 )","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:54:26.113501Z","iopub.execute_input":"2023-06-04T04:54:26.113777Z","iopub.status.idle":"2023-06-04T04:54:44.555744Z","shell.execute_reply.started":"2023-06-04T04:54:26.113746Z","shell.execute_reply":"2023-06-04T04:54:44.554901Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca291789de84b368fabddf6e5985187"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train();","metadata":{"execution":{"iopub.status.busy":"2023-06-04T04:54:45.513161Z","iopub.execute_input":"2023-06-04T04:54:45.513960Z","iopub.status.idle":"2023-06-04T05:14:50.045135Z","shell.execute_reply.started":"2023-06-04T04:54:45.513906Z","shell.execute_reply":"2023-06-04T05:14:50.044287Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text_length, discourse_type, discourse_id, discourse_text, input, discourse_effectiveness, essay_id.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 27573\n  Num Epochs = 3\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 2586\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2586' max='2586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2586/2586 19:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Loss</th>\n      <th>Accuracy</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n      <th>Steps Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.809100</td>\n      <td>0.803800</td>\n      <td>0.803885</td>\n      <td>0.657400</td>\n      <td>44.075300</td>\n      <td>208.552000</td>\n      <td>3.267000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.628800</td>\n      <td>0.713100</td>\n      <td>0.713139</td>\n      <td>0.692000</td>\n      <td>43.933500</td>\n      <td>209.225000</td>\n      <td>3.278000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.357700</td>\n      <td>0.886900</td>\n      <td>0.886987</td>\n      <td>0.678100</td>\n      <td>43.921000</td>\n      <td>209.285000</td>\n      <td>3.279000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1410: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\nSaving model checkpoint to outputs/checkpoint-500\nConfiguration saved in outputs/checkpoint-500/config.json\nModel weights saved in outputs/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThe following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text_length, discourse_type, discourse_id, discourse_text, input, discourse_effectiveness, essay_id.\n***** Running Evaluation *****\n  Num examples = 9192\n  Batch size = 64\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to outputs/checkpoint-1000\nConfiguration saved in outputs/checkpoint-1000/config.json\nModel weights saved in outputs/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to outputs/checkpoint-1500\nConfiguration saved in outputs/checkpoint-1500/config.json\nModel weights saved in outputs/checkpoint-1500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-1500/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThe following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text_length, discourse_type, discourse_id, discourse_text, input, discourse_effectiveness, essay_id.\n***** Running Evaluation *****\n  Num examples = 9192\n  Batch size = 64\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to outputs/checkpoint-2000\nConfiguration saved in outputs/checkpoint-2000/config.json\nModel weights saved in outputs/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSaving model checkpoint to outputs/checkpoint-2500\nConfiguration saved in outputs/checkpoint-2500/config.json\nModel weights saved in outputs/checkpoint-2500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-2500/special_tokens_map.json\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThe following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text_length, discourse_type, discourse_id, discourse_text, input, discourse_effectiveness, essay_id.\n***** Running Evaluation *****\n  Num examples = 9192\n  Batch size = 64\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
